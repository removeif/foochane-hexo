---
title: Spark的安装及配置
summary: Hadoop集群环境下，Spark的安装和配置
date: 2019-5-18 13:56
author: foochane
urlname: 2019051904
categories: 大数据
tags:
  - hadoop
  - 大数据
---




## 1 安装Scala

### 1.1 解压安装包的安装目录

```
$ mkdir /usr/local/bigdata/scala
$ tar zxvf scala-2.12.5.tgz -C /usr/local/bigdata/scala
```
### 1.2 添加环境变量，病使其生效。
```
$ sudo vim  ~/.bashrc
```
添加
```
export SCALA_HOME=/usr/local/bigdata/scala/scala-2.12.5
export PATH=/usr/local/bigdata/scala/scala-2.12.5/bin:$PATH
```
然后
```
$ source ~/.bashrc
```
### 1.3 测试
```
scala -version

Scala code runner version 2.12.5 -- Copyright 2002-2018, LAMP/EPFL and Lightbe
```

## 2 安装Spark

### 2.1 解压安装包的安装目录

```
$ tar zxvf spark-2.4.1-bin-hadoop2.7.tgz -C /usr/local/bigdata/
$ cd /usr/local/bigdata/
$ mv spark-2.4.1-bin-hadoop2.7 spark
```

```
$ cd ./conf
$ cp spark-env.sh.template spark-env.sh
cp slaves.template slaves
```

### 2.2 修改配置文件
#### (1) spark-env.sh

添加如下内容

```
export SCALA_HOME=/usr/local/bigdata/scala
export JAVA_HOME=/usr/local/bigdata/java/jdk1.8.0_60
export HADOOP_HOME=/usr/local/bigdata/hadoop/
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
SPARK_MASTER_IP=master
SPARK_LOCAL_DIRS=/usr/local/bigdata/spark
SPARK_DRIVER_MEMORY=512M
```

#### (2)slaves

添加
```
Slave1
Slave2
```

### 2.3 配置环境变量

```
# spark
export PATH=$PATH:/usr/local/bigdata/spark/bin:/usr/local/bigdata/spark/sbin
```

## 3 运行Spark

```
# 先启动hadoop
$ cd /usr/local/hadoop/bigdata/sbin/

$ ./start-dfs.sh
$ ./start-yarn.sh
#$ ./start-history-server.sh

# 启动sapark
$ cd /usr/local/bigdata/spark/sbin/

$ ./start-all.sh
$ ./start-history-server.sh

````

浏览器查看相关资源情况：http://master:8080/
